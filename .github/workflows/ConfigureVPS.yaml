name: ConfigureVPS

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - host: k8s-cluster-main
            system: aarch64-linux
            ip: K8S_CLUSTER_MAIN_IP
            username: K8S_CLUSTER_MAIN_USER
            password: K8S_CLUSTER_MAIN_PASSWORD
          - host: k8s-cluster-node-1
            system: aarch64-linux
            ip: K8S_CLUSTER_NODE1_IP
            username: K8S_CLUSTER_NODE1_USER
            password: K8S_CLUSTER_NODE1_PASSWORD

    steps:
      - uses: actions/checkout@v3

      - name: Install Nix CLI locally
        uses: cachix/install-nix-action@v22
        with:
          nix_path: nixpkgs=channel:nixos-23.11

      - name: Enable flakes & allow ARM builds locally
        run: |
          mkdir -p ~/.config/nix
          cat > ~/.config/nix/nix.conf <<'EOF'
          experimental-features = nix-command flakes
          extra-platforms = aarch64-linux x86_64-linux
          EOF

      - name: Enable aarch64 emulation on runner
        run: |
          sudo apt-get update
          sudo apt-get install -y qemu-user-static binfmt-support
          sudo update-binfmt --enable qemu-aarch64 || true

      # 1) Bootstrap Nix on the remote, if needed
      - name: Bootstrap Nix on ${{ matrix.host }}
        uses: appleboy/ssh-action@v0.1.7
        with:
          host: ${{ secrets[matrix.ip] }}
          username: ${{ secrets[matrix.username] }}
          password: ${{ secrets[matrix.password] }}
          script: |
            if ! command -v nix >/dev/null 2>&1; then
              echo "Nix not found ➜ installing…"

              # clean up artefacts of aborted installs to keep script idempotent
              find /etc -maxdepth 1 -name '*backup-before-nix' -print -exec rm -f {} +

              # install deb dependencies
              apt-get update
              apt-get install -y xz-utils curl sudo

              # non-interactive, daemon mode
              sh <(curl --proto '=https' --tlsv1.2 -L https://nixos.org/nix/install) --daemon --yes
            fi
            # ensure required nix.conf settings
            mkdir -p /etc/nix
            cat >/etc/nix/nix.conf <<'EON'
            experimental-features = nix-command flakes
            require-sigs = false
            trusted-users = root ${{ secrets[matrix.username] }}
            EON

            # ensure user profile bin is on PATH for future login shells
            cat >/etc/profile.d/nix-user-profile.sh <<'EOP'
            if [ -d "$HOME/.nix-profile/bin" ]; then
              PATH="$HOME/.nix-profile/bin:$PATH"
            fi
            export PATH
            EOP
            chmod 644 /etc/profile.d/nix-user-profile.sh

      # 2) Build the flake output for this host
      - name: Build flake for ${{ matrix.host }}
        id: build
        run: |
          nix build .#packages.${{ matrix.system }}.${{ matrix.host }} -o result
          echo "STORE_PATH=$(readlink -f result)" >> "$GITHUB_ENV"

      # 1.5) Set up SSH key for passwordless copy
      - name: Set up SSH key for copy
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      # 3) Push the closure to the remote
      - name: Copy closure to ${{ matrix.host }}
        env:
          SSH_USER: ${{ secrets[matrix.username] }}
          SSH_HOST: ${{ secrets[matrix.ip] }}
          NIX_SSHOPTS: "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
          STORE_PATH: ${{ env.STORE_PATH }}
        run: |
          # Use the newer ssh-ng transport which keeps a single persistent
          # control connection instead of spawning hundreds of short ssh
          # sessions (which can be throttled by the VPS or interrupted by
          # GitHub runners).  Add ControlMaster to reuse the socket.
          export NIX_SSHOPTS="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ControlMaster=auto -o ControlPersist=5m -o ControlPath=/tmp/ssh-%r@%h:%p"
          nix copy --no-check-sigs --to "ssh-ng://$SSH_USER@$SSH_HOST" "$STORE_PATH" &
          COPY_PID=$!
          while kill -0 "$COPY_PID" 2>/dev/null; do
            echo "still copying… (pid $COPY_PID)"
            sleep 30
          done
          wait "$COPY_PID"

      # 4) Activate on the remote
      - name: Activate on ${{ matrix.host }}
        uses: appleboy/ssh-action@v0.1.7
        with:
          host: ${{ secrets[matrix.ip] }}
          username: ${{ secrets[matrix.username] }}
          password: ${{ secrets[matrix.password] }}
          envs: STORE_PATH
          script: |
            # ensure nix is on PATH for this non-login shell
            source /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh || true
            echo "Activating $STORE_PATH…"
            nix-env --profile $HOME/.nix-profile --set "$STORE_PATH"

      # 5) Provision and start k3s (server on control-plane, agent on workers)
      - name: Configure k3s on ${{ matrix.host }}
        uses: appleboy/ssh-action@v0.1.7
        env:
          HOST_NAME: ${{ matrix.host }}
          K3S_TOKEN: ${{ secrets.K3S_CLUSTER_TOKEN }}
          SERVER_IP: ${{ secrets.K8S_CLUSTER_MAIN_IP }}
        with:
          host: ${{ secrets[matrix.ip] }}
          username: ${{ secrets[matrix.username] }}
          password: ${{ secrets[matrix.password] }}
          envs: HOST_NAME,K3S_TOKEN,SERVER_IP
          script: |
            set -e

            # ensure nix profile binaries are on PATH for this non-login shell
            source /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh || true

            # Path to the k3s binary inside the nix profile
            K3S_BIN="$HOME/.nix-profile/bin/k3s"

            if [ ! -x "$K3S_BIN" ]; then
              echo "k3s binary not found in nix profile ($K3S_BIN)" >&2
              exit 1
            fi

            # Use the official k3s install script to generate/update the systemd unit.
            # We SKIP the binary download so it reuses the one provided by Nix.
            INSTALL_ENV="INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_BIN_DIR=$HOME/.nix-profile/bin"

            if [ "$HOST_NAME" = "k8s-cluster-main" ]; then
              echo "### Installing/updating k3s server service"
              curl -sfL https://get.k3s.io | \
                env INSTALL_K3S_SKIP_DOWNLOAD=true \
                    INSTALL_K3S_BIN_DIR="$HOME/.nix-profile/bin" \
                    K3S_TOKEN="$K3S_TOKEN" \
                    sh -s - server --write-kubeconfig-mode 644 --disable traefik --disable metrics-server
            else
              echo "### Installing/updating k3s agent service"
              curl -sfL https://get.k3s.io | \
                env INSTALL_K3S_SKIP_DOWNLOAD=true \
                    INSTALL_K3S_BIN_DIR="$HOME/.nix-profile/bin" \
                    K3S_URL="https://$SERVER_IP:6443" \
                    K3S_TOKEN="$K3S_TOKEN" \
                    sh -s - agent
            fi

            echo "k3s systemd service ready on $HOST_NAME"

  deploy-ingress:
    runs-on: ubuntu-latest
    needs: deploy
    steps:
      - name: Deploy Traefik and cert-manager
        uses: appleboy/ssh-action@v0.1.7
        env:
          K8S_CLUSTER_MAIN_IP: ${{ secrets.K8S_CLUSTER_MAIN_IP }}
          K8S_CLUSTER_MAIN_USER: ${{ secrets.K8S_CLUSTER_MAIN_USER }}
          K8S_CLUSTER_MAIN_PASSWORD: ${{ secrets.K8S_CLUSTER_MAIN_PASSWORD }}
          LETSENCRYPT_EMAIL: ${{ secrets.LETSENCRYPT_EMAIL }}
        with:
          host: ${{ secrets.K8S_CLUSTER_MAIN_IP }}
          username: ${{ secrets.K8S_CLUSTER_MAIN_USER }}
          password: ${{ secrets.K8S_CLUSTER_MAIN_PASSWORD }}
          envs: K8S_CLUSTER_MAIN_IP,K8S_CLUSTER_MAIN_USER,K8S_CLUSTER_MAIN_PASSWORD,LETSENCRYPT_EMAIL
          script: |
            set -e

            echo "=== Starting Traefik and cert-manager deployment ==="

            # Wait for k3s to be ready
            echo "Waiting for k3s to be ready..."
            timeout 300 bash -c 'until kubectl get nodes 2>/dev/null; do sleep 5; done' || {
              echo "Error: k3s not ready after 5 minutes"
              exit 1
            }

            # Ensure kubectl is available and kubeconfig is set up
            if ! command -v kubectl >/dev/null 2>&1; then
              echo "Creating kubectl symlink..."
              sudo ln -sf /usr/local/bin/k3s /usr/local/bin/kubectl
            fi

            # Set up kubeconfig for the current user
            echo "Setting up kubeconfig..."
            mkdir -p ~/.kube
            sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
            sudo chown $USER:$USER ~/.kube/config
            chmod 600 ~/.kube/config

            # Test kubectl connection
            echo "Testing kubectl connection..."
            kubectl get nodes

                        # Download and install official Helm binary (avoid Nix-built version)
            echo "Installing official Helm binary..."
            HELM_VERSION="v3.14.2"
            HELM_DIR="/tmp/helm-${HELM_VERSION}"

            # Remove any existing Helm from PATH to avoid Nix-built version
            export PATH="/usr/local/bin:/usr/bin:/bin"

            # Download and install Helm
            mkdir -p "$HELM_DIR"
            cd "$HELM_DIR"
            curl -fsSL -o helm.tar.gz "https://get.helm.sh/helm-${HELM_VERSION}-linux-arm64.tar.gz"
            tar -xzf helm.tar.gz
            cd linux-arm64
            sudo cp helm /usr/local/bin/
            sudo chmod +x /usr/local/bin/helm
            cd /
            rm -rf "$HELM_DIR"

            # Verify Helm installation
            /usr/local/bin/helm version --client

            # Install Traefik
            echo "Installing Traefik..."
            /usr/local/bin/helm repo add traefik https://traefik.github.io/charts
            /usr/local/bin/helm repo update

            # Check if Traefik is already installed
            if ! /usr/local/bin/helm list -n kube-system --kubeconfig ~/.kube/config | grep -q traefik; then
              /usr/local/bin/helm install traefik traefik/traefik \
                --namespace kube-system \
                --create-namespace \
                --version 25.0.0 \
                --set ports.web.redirectTo.port=websecure \
                --set ports.websecure.tls.enabled=true \
                --kubeconfig ~/.kube/config
              echo "Traefik installed successfully"
            else
              echo "Traefik already installed, upgrading..."
              /usr/local/bin/helm upgrade traefik traefik/traefik \
                --namespace kube-system \
                --version 25.0.0 \
                --set ports.web.redirectTo.port=websecure \
                --set ports.websecure.tls.enabled=true \
                --kubeconfig ~/.kube/config
            fi

            # Wait for Traefik to be ready
            echo "Waiting for Traefik to be ready..."
            kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=traefik -n kube-system --timeout=300s

            # Install cert-manager
            echo "Installing cert-manager..."
            /usr/local/bin/helm repo add jetstack https://charts.jetstack.io
            /usr/local/bin/helm repo update

            # Check if cert-manager is already installed
            if ! /usr/local/bin/helm list -n cert-manager --kubeconfig ~/.kube/config | grep -q cert-manager; then
              /usr/local/bin/helm install cert-manager jetstack/cert-manager \
                --namespace cert-manager \
                --create-namespace \
                --version v1.13.3 \
                --set installCRDs=true \
                --kubeconfig ~/.kube/config
              echo "cert-manager installed successfully"
            else
              echo "cert-manager already installed, upgrading..."
              /usr/local/bin/helm upgrade cert-manager jetstack/cert-manager \
                --namespace cert-manager \
                --version v1.13.3 \
                --set installCRDs=true \
                --kubeconfig ~/.kube/config
            fi

            # Wait for cert-manager to be ready
            echo "Waiting for cert-manager to be ready..."
            kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=cert-manager -n cert-manager --timeout=300s

            # Create ClusterIssuer for Let's Encrypt
            echo "Creating ClusterIssuer..."
            kubectl apply -f - <<EOF
            apiVersion: cert-manager.io/v1
            kind: ClusterIssuer
            metadata:
              name: letsencrypt-prod
            spec:
              acme:
                server: https://acme-v02.api.letsencrypt.org/directory
                email: $LETSENCRYPT_EMAIL
                privateKeySecretRef:
                  name: letsencrypt-prod
                solvers:
                - http01:
                    ingress:
                      class: traefik
            EOF

            # Wait for ClusterIssuer to be ready
            echo "Waiting for ClusterIssuer to be ready..."
            timeout 120 bash -c 'until kubectl get clusterissuer letsencrypt-prod -o jsonpath="{.status.conditions[?(@.type==\"Ready\")].status}" | grep -q "True"; do sleep 5; done' || {
              echo "Warning: ClusterIssuer not ready after 2 minutes, but continuing..."
            }

            # Verify installation
            echo "=== Verification ==="
            echo "Traefik pods:"
            kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik
            echo ""
            echo "cert-manager pods:"
            kubectl get pods -n cert-manager
            echo ""
            echo "ClusterIssuer:"
            kubectl get clusterissuer letsencrypt-prod
            echo ""
            echo "=== Traefik and cert-manager deployment completed successfully ==="

            # Set up multi-environment infrastructure
            echo "=== Setting up multi-environment infrastructure ==="

            # Create environment namespaces
            echo "Creating environment namespaces..."

            # Check if namespaces already exist and create them
            echo "Creating environment namespaces..."

            # Create production namespace
            if kubectl get namespace production >/dev/null 2>&1; then
              echo "Production namespace already exists"
            else
              echo "Creating production namespace..."
              kubectl create namespace production --dry-run=client -o yaml | kubectl apply -f -
              kubectl label namespace production name=production environment=production
            fi

            # Create stage namespace
            if kubectl get namespace stage >/dev/null 2>&1; then
              echo "Stage namespace already exists"
            else
              echo "Creating stage namespace..."
              kubectl create namespace stage --dry-run=client -o yaml | kubectl apply -f -
              kubectl label namespace stage name=stage environment=stage
            fi

            # Show namespace status
            echo "Current namespace status:"
            kubectl get namespaces | grep -E "(production|stage)"

            # Wait for namespaces to be ready
            echo "Waiting for namespaces to be ready..."

            # Wait for production namespace with better error handling
            echo "Waiting for production namespace..."
            timeout 120 bash -c 'until kubectl get namespace production -o jsonpath="{.status.phase}" | grep -q "Active"; do sleep 2; done' || {
              echo "Warning: Production namespace not ready after 2 minutes, but continuing..."
              kubectl describe namespace production
            }

            # Wait for stage namespace with better error handling
            echo "Waiting for stage namespace..."
            timeout 120 bash -c 'until kubectl get namespace stage -o jsonpath="{.status.phase}" | grep -q "Active"; do sleep 2; done' || {
              echo "Warning: Stage namespace not ready after 2 minutes, but continuing..."
              kubectl describe namespace stage
            }

            echo "Namespace status after waiting:"
            kubectl get namespaces | grep -E "(production|stage)"

            # Deploy base ingress configurations for environments
            echo "Deploying base ingress configurations..."

            # Production ingress
            kubectl apply -f - <<EOF
            apiVersion: networking.k8s.io/v1
            kind: Ingress
            metadata:
              name: production-ingress
              namespace: production
              annotations:
                cert-manager.io/cluster-issuer: "letsencrypt-prod"
                traefik.ingress.kubernetes.io/router.entrypoints: websecure
                traefik.ingress.kubernetes.io/router.tls: "true"
                traefik.ingress.kubernetes.io/rate-limit: "100"
            spec:
              tls:
                - hosts:
                    - juliosampaio.com
                  secretName: production-tls
              rules:
                - host: juliosampaio.com
                  http:
                    paths:
                      - path: /
                        pathType: Prefix
                        backend:
                          service:
                            name: main-website-service
                            port:
                              number: 80
            EOF

            # Stage ingress
            kubectl apply -f - <<EOF
            apiVersion: networking.k8s.io/v1
            kind: Ingress
            metadata:
              name: stage-ingress
              namespace: stage
              annotations:
                cert-manager.io/cluster-issuer: "letsencrypt-prod"
                traefik.ingress.kubernetes.io/router.entrypoints: websecure
                traefik.ingress.kubernetes.io/router.tls: "true"
                traefik.ingress.kubernetes.io/rate-limit: "50"
            spec:
              tls:
                - hosts:
                    - stage.juliosampaio.com
                  secretName: stage-tls
              rules:
                - host: stage.juliosampaio.com
                  http:
                    paths:
                      - path: /
                        pathType: Prefix
                        backend:
                          service:
                            name: main-website-service
                            port:
                              number: 80
            EOF

            # Verify environment setup
            echo "=== Environment Setup Verification ==="
            echo "Namespaces:"
            kubectl get namespaces | grep -E "(production|stage)"
            echo ""
            echo "Production ingress:"
            kubectl get ingress -n production
            echo ""
            echo "Stage ingress:"
            kubectl get ingress -n stage
            echo ""
            echo "=== Multi-environment infrastructure setup completed successfully ==="

            # Deploy shared PostgreSQL databases
            echo "=== Deploying shared PostgreSQL databases ==="

            # Check if PostgreSQL secret already exists and get existing password
            if kubectl get secret postgres-secret -n production >/dev/null 2>&1; then
              echo "PostgreSQL secret already exists - using existing password"
              POSTGRES_PASSWORD_B64=$(kubectl get secret postgres-secret -n production -o jsonpath='{.data.postgres-password}')
              POSTGRES_PASSWORD=$(echo -n "$POSTGRES_PASSWORD_B64" | base64 -d)
              echo "Using existing PostgreSQL password"
            else
              echo "No existing PostgreSQL secret found - generating new password"
              POSTGRES_PASSWORD=$(openssl rand -base64 32)
              POSTGRES_PASSWORD_B64=$(echo -n "$POSTGRES_PASSWORD" | base64)
              echo "Generated new secure PostgreSQL password"
              echo "PostgreSQL Password: $POSTGRES_PASSWORD"
              echo "⚠️  Save this password securely - it won't be shown again!"
            fi

            # Function to deploy PostgreSQL to an environment
            deploy_postgres() {
                local environment=$1
                echo ""
                echo "Deploying PostgreSQL to $environment environment..."
                
                # Create environment-specific deployment files
                PROD_FILE="/tmp/postgres-${environment}.yaml"
                
                # Create PostgreSQL deployment for the environment
                cat > "$PROD_FILE" <<POSTGRES_EOF
            apiVersion: v1
            kind: Secret
            metadata:
              name: postgres-secret
              namespace: $environment
            type: Opaque
            data:
              postgres-password: $POSTGRES_PASSWORD_B64
            ---
            apiVersion: v1
            kind: ConfigMap
            metadata:
              name: postgres-config
              namespace: $environment
            data:
              POSTGRES_DB: "shared_db"
              POSTGRES_USER: "postgres"
            ---
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: postgres
              namespace: $environment
              labels:
                app: postgres
                component: database
            spec:
              replicas: 1
              selector:
                matchLabels:
                  app: postgres
              template:
                metadata:
                  labels:
                    app: postgres
                spec:
                  containers:
                  - name: postgres
                    image: postgres:15-alpine
                    ports:
                    - containerPort: 5432
                      name: postgres
                    env:
                    - name: POSTGRES_PASSWORD
                      valueFrom:
                        secretKeyRef:
                          name: postgres-secret
                          key: postgres-password
                    - name: POSTGRES_DB
                      valueFrom:
                        configMapKeyRef:
                          name: postgres-config
                          key: POSTGRES_DB
                    - name: POSTGRES_USER
                      valueFrom:
                        configMapKeyRef:
                          name: postgres-config
                          key: POSTGRES_USER
                    - name: PGDATA
                      value: /var/lib/postgresql/data/pgdata
                    volumeMounts:
                    - name: postgres-storage
                      mountPath: /var/lib/postgresql/data
                    resources:
                      requests:
                        memory: "256Mi"
                        cpu: "250m"
                      limits:
                        memory: "512Mi"
                        cpu: "500m"
                    livenessProbe:
                      exec:
                        command:
                        - pg_isready
                        - -U
                        - postgres
                      initialDelaySeconds: 30
                      periodSeconds: 10
                    readinessProbe:
                      exec:
                        command:
                        - pg_isready
                        - -U
                        - postgres
                      initialDelaySeconds: 5
                      periodSeconds: 5
                  volumes:
                  - name: postgres-storage
                    persistentVolumeClaim:
                      claimName: postgres-pvc
            ---
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: postgres-pvc
              namespace: $environment
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 10Gi
              storageClassName: local-path
            ---
            apiVersion: v1
            kind: Service
            metadata:
              name: postgres-service
              namespace: $environment
              labels:
                app: postgres
                component: database
            spec:
              selector:
                app: postgres
              ports:
              - port: 5432
                targetPort: 5432
                name: postgres
              type: ClusterIP
            POSTGRES_EOF
                
                # Apply the deployment
                kubectl apply -f "$PROD_FILE"
                
                # Wait for PostgreSQL to be ready
                echo "Waiting for PostgreSQL to be ready in $environment..."
                kubectl wait --for=condition=available deployment/postgres -n $environment --timeout=300s
                
                # Verify the deployment
                echo "Verifying PostgreSQL deployment in $environment..."
                kubectl get pods -n $environment -l app=postgres
                kubectl get services -n $environment -l app=postgres
                
                # Test database connection
                echo "Testing database connection in $environment..."
                kubectl exec -n $environment deployment/postgres -- pg_isready -U postgres
                
                # Clean up temporary file
                rm -f "$PROD_FILE"
                
                echo "PostgreSQL deployment completed for $environment"
            }

            # Deploy to production
            deploy_postgres "production"

            # Deploy to stage
            deploy_postgres "stage"

            # Deploy backup endpoints for n8n
            echo ""
            echo "=== Deploying backup endpoints for n8n ==="

            # Create backup endpoints directly (since file might not be available on remote)
            kubectl apply -f - <<EOF
            apiVersion: v1
            kind: Service
            metadata:
              name: postgres-backup-service
              namespace: production
              labels:
                app: postgres
                component: database
                purpose: backup
            spec:
              selector:
                app: postgres
              ports:
              - port: 5432
                targetPort: 5432
                name: postgres
                protocol: TCP
              type: ClusterIP
            ---
            apiVersion: v1
            kind: Service
            metadata:
              name: postgres-backup-service
              namespace: stage
              labels:
                app: postgres
                component: database
                purpose: backup
            spec:
              selector:
                app: postgres
              ports:
              - port: 5432
                targetPort: 5432
                name: postgres
                protocol: TCP
              type: ClusterIP
            EOF

            echo ""
            echo "=== PostgreSQL deployment completed successfully ==="
            echo ""
            echo "Database connection details:"
            echo ""
            echo "Production:"
            echo "  Host: postgres-service.production.svc.cluster.local"
            echo "  Port: 5432"
            echo "  Database: shared_db"
            echo "  Username: postgres"
            echo ""
            echo "Stage:"
            echo "  Host: postgres-service.stage.svc.cluster.local"
            echo "  Port: 5432"
            echo "  Database: shared_db"
            echo "  Username: postgres"
            echo ""
            echo "The cluster is now ready for external application deployments!"
            echo "External repositories can deploy applications using:"
            echo "  kubectl apply -f app-deployment.yaml -n production"
            echo "  kubectl apply -f app-deployment.yaml -n stage"
            echo ""
            echo "To create databases for applications, use:"
            echo "  ./k8s/environments/database/create-app-database.sh <app-name> <environment>"
            echo ""
            echo "Examples:"
            echo "  ./k8s/environments/database/create-app-database.sh myapp production"
            echo "  ./k8s/environments/database/create-app-database.sh myapp stage"
            echo ""
            echo "Or using the provided Helm chart:"
            echo "  helm upgrade --install app-name k8s/environments/helm-charts/app-template --namespace production"
